{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: the loss is 2.2870299221038817\n",
      "epoch 1: the loss is 0.9939679298559825\n",
      "epoch 2: the loss is 0.2069957865158717\n",
      "epoch 3: the loss is 0.13284637599885463\n",
      "epoch 4: the loss is 0.10100925095478694\n",
      "epoch 5: the loss is 0.08576445230493943\n",
      "epoch 6: the loss is 0.07197539932057262\n",
      "epoch 7: the loss is 0.06616005884259939\n",
      "epoch 8: the loss is 0.05883001445891956\n",
      "epoch 9: the loss is 0.051704800932481886\n",
      "epoch 10: the loss is 0.050114954197655125\n",
      "epoch 11: the loss is 0.044661494140326975\n",
      "epoch 12: the loss is 0.04308367066072921\n",
      "epoch 13: the loss is 0.039925323380095266\n",
      "epoch 14: the loss is 0.03847170854757229\n",
      "epoch 15: the loss is 0.03642655095333854\n",
      "epoch 16: the loss is 0.03397341289985925\n",
      "epoch 17: the loss is 0.032936949378686645\n",
      "epoch 18: the loss is 0.03139347783382982\n",
      "epoch 19: the loss is 0.028623986722280583\n",
      "the loss on test data is 0.05299294187807286\n",
      "The accuracy on the testing data is 0.9807\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from FileLoader import *\n",
    "\n",
    "batch_size = 64\n",
    "num_epoch = 20\n",
    "num_feature_channel = 32\n",
    "\n",
    "train_images, train_labels = load_train_data()\n",
    "full_train_data = to_tensor(train_images)[0, :, :]\n",
    "full_train_target = []\n",
    "for i in range(60000):\n",
    "    full_train_target.append(np.zeros(10))\n",
    "for i in range(60000):\n",
    "    k = train_labels[i]\n",
    "    full_train_target[i][k] = 1\n",
    "full_train_target = np.array(full_train_target)\n",
    "full_train_target = torch.from_numpy(full_train_target)\n",
    "\n",
    "\n",
    "test_images, test_labels = load_test_data()\n",
    "full_test_data = to_tensor(test_images)[0, :, :]\n",
    "full_test_target = []\n",
    "for i in range(10000):\n",
    "    full_test_target.append(np.zeros(10))\n",
    "for i in range(10000):\n",
    "    k = test_labels[i]\n",
    "    full_test_target[i][k] = 1\n",
    "full_test_target = np.array(full_test_target)\n",
    "full_test_target = torch.from_numpy(full_test_target)\n",
    "\n",
    "full_train_data = full_train_data.reshape(60000, 1, 28, 28)\n",
    "full_test_data = full_test_data.reshape(10000, 1, 28, 28)\n",
    "\n",
    "\n",
    "class CAN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CAN, self).__init__()\n",
    "\n",
    "        # Feature map size = 28+2*padding-(2*dilation+1)+1 = 28\n",
    "        # so the size of feature maps retain the size of input image, ie: 28\n",
    "        self.conv1 = torch.nn.Conv2d(1, num_feature_channel, 3, dilation=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(num_feature_channel, num_feature_channel, 3, dilation=2, padding=2)\n",
    "        self.conv3 = torch.nn.Conv2d(num_feature_channel, num_feature_channel, 3, dilation=4, padding=4)\n",
    "        self.conv4 = torch.nn.Conv2d(num_feature_channel, num_feature_channel, 3, dilation=8, padding=8)\n",
    "        self.conv5 = torch.nn.Conv2d(num_feature_channel, 10, 3, dilation=1, padding=1)\n",
    "        self.pool = torch.nn.AvgPool2d(28)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = torch.nn.functional.leaky_relu(self.conv1(X))\n",
    "        X = torch.nn.functional.leaky_relu(self.conv2(X))\n",
    "        X = torch.nn.functional.leaky_relu(self.conv3(X))\n",
    "        X = torch.nn.functional.leaky_relu(self.conv4(X))\n",
    "        X = torch.nn.functional.leaky_relu(self.conv5(X))\n",
    "        X = self.pool(X)\n",
    "        X = torch.flatten(X, 1)\n",
    "        return X\n",
    "\n",
    "\n",
    "can = CAN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(can.parameters(), lr=0.1)\n",
    "\n",
    "# the trainning loop\n",
    "for epoch in range(num_epoch):  # 20 epochs\n",
    "    can.train()\n",
    "    permutation = torch.randperm(60000)\n",
    "    train_loss_in_this_epoch = 0.0\n",
    "\n",
    "    # batch size of 64, takes 938 iterations to go through whole dataset\n",
    "    for batch in range(60000//batch_size + 1):\n",
    "        if batch != 60000//batch_size:\n",
    "            this_batch_size = batch_size\n",
    "            input = np.zeros(batch_size*28*28).reshape(batch_size, 1, 28, 28)\n",
    "            input = torch.from_numpy(input)\n",
    "            for i in range(batch_size):\n",
    "                input[i] = full_train_data[permutation[batch*batch_size+i]]\n",
    "            input = input.to(torch.float32)\n",
    "\n",
    "            target = np.zeros(batch_size*10).reshape(batch_size, 10)\n",
    "            target = torch.from_numpy(target)\n",
    "            for i in range(batch_size):\n",
    "                target[i] = full_train_target[permutation[batch*batch_size+i]]\n",
    "            target = target.to(torch.float32)\n",
    "\n",
    "        else:\n",
    "            this_batch_size = 60000-batch*batch_size\n",
    "            input = np.zeros(this_batch_size*28 *\n",
    "                             28).reshape(this_batch_size, 1, 28, 28)\n",
    "            for i in range(this_batch_size):\n",
    "                input[i] = full_train_data[permutation[batch*batch_size+i]]\n",
    "            input = torch.from_numpy(input)\n",
    "            input = input.to(torch.float32)\n",
    "\n",
    "            target = np.zeros(this_batch_size*10).reshape(this_batch_size, 10)\n",
    "            for i in range(this_batch_size):\n",
    "                target[i] = full_train_target[permutation[batch*batch_size+i]]\n",
    "            target = torch.from_numpy(target)\n",
    "            target = target.to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = can(input)\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_in_this_epoch += loss.item()*this_batch_size\n",
    "\n",
    "    print(\"epoch\", epoch, end=\": \")\n",
    "    print(\"the loss is\", train_loss_in_this_epoch/60000)\n",
    "\n",
    "test_prediction = can(full_test_data)\n",
    "test_loss = criterion(test_prediction, full_test_target)\n",
    "print(\"the loss on test data is\", test_loss.item())\n",
    "\n",
    "predicted_numbers = []\n",
    "for i in range(10000):\n",
    "    predicted_number = torch.argmax(test_prediction[i]).item()\n",
    "    predicted_numbers.append(predicted_number)\n",
    "predicted_numbers = np.array(predicted_numbers)\n",
    "\n",
    "num_error = 0\n",
    "for truth, prediction in zip(test_labels, predicted_numbers):\n",
    "    if truth != prediction:\n",
    "        num_error += 1\n",
    "print(\"The accuracy on the testing data is\", 1-num_error/10000)\n",
    "\n",
    "###\n",
    "# feature channel=4: 0.9551\n",
    "# feature channel=8: 0.9739\n",
    "# feature channel=16: 0.9891\n",
    "# feature channel=32: 0.9807\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea7655f8563b6b304d5ebbeaa9fbbcccb7096429f906efd7b44913c32e5b88a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
