{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: the loss is 1.7148094414393107\n",
      "epoch 1: the loss is 0.5396497692584992\n",
      "epoch 2: the loss is 0.38467589570681254\n",
      "epoch 3: the loss is 0.33608804298241934\n",
      "epoch 4: the loss is 0.30700241635640463\n",
      "epoch 5: the loss is 0.28479987699985504\n",
      "epoch 6: the loss is 0.2661201255480448\n",
      "epoch 7: the loss is 0.24984456133842467\n",
      "epoch 8: the loss is 0.23494837313890457\n",
      "epoch 9: the loss is 0.221769046942393\n",
      "epoch 10: the loss is 0.2094221082687378\n",
      "epoch 11: the loss is 0.19849226214885712\n",
      "epoch 12: the loss is 0.1883178782224655\n",
      "epoch 13: the loss is 0.17846401159763337\n",
      "epoch 14: the loss is 0.17013654433290162\n",
      "epoch 15: the loss is 0.16199375006159147\n",
      "epoch 16: the loss is 0.15453088796933492\n",
      "epoch 17: the loss is 0.1480057725816965\n",
      "epoch 18: the loss is 0.14147629124919572\n",
      "epoch 19: the loss is 0.13559403726259867\n",
      "the loss on test data is 0.1377743642184878\n",
      "The accuracy on the testing data is 0.9594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from FileLoader import *\n",
    "\n",
    "batch_size = 64\n",
    "num_epoch = 20\n",
    "# adjust to plot a curve of accuracy versus the number of neurons: 4, 8, 16, 32, 64, 128, and 256\n",
    "num_of_neurons_in_hidden = 256\n",
    "\n",
    "\n",
    "train_images, train_labels = load_train_data()\n",
    "full_train_data = to_tensor(train_images)[0, :, :]\n",
    "full_train_target = []\n",
    "for i in range(60000):\n",
    "    full_train_target.append(np.zeros(10))\n",
    "for i in range(60000):\n",
    "    k = train_labels[i]\n",
    "    full_train_target[i][k] = 1\n",
    "full_train_target = np.array(full_train_target)\n",
    "full_train_target = torch.from_numpy(full_train_target)\n",
    "\n",
    "\n",
    "test_images, test_labels = load_test_data()\n",
    "full_test_data = to_tensor(test_images)[0, :, :]\n",
    "full_test_target = []\n",
    "for i in range(10000):\n",
    "    full_test_target.append(np.zeros(10))\n",
    "for i in range(10000):\n",
    "    k = test_labels[i]\n",
    "    full_test_target[i][k] = 1\n",
    "full_test_target = np.array(full_test_target)\n",
    "full_test_target = torch.from_numpy(full_test_target)\n",
    "\n",
    "\n",
    "class MLPClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fcl1 = torch.nn.Linear(28*28, num_of_neurons_in_hidden)\n",
    "        self.fcl2 = torch.nn.Linear(\n",
    "            num_of_neurons_in_hidden, num_of_neurons_in_hidden)\n",
    "        self.fcl3 = torch.nn.Linear(num_of_neurons_in_hidden, 10)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.fcl1(X)\n",
    "        X = torch.nn.functional.relu(X)\n",
    "        X = self.fcl2(X)\n",
    "        X = torch.nn.functional.relu(X)\n",
    "        X = self.fcl3(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epoch):  # 20 epochs\n",
    "    mlp.train()\n",
    "    permutation = torch.randperm(60000)\n",
    "    train_loss_in_this_epoch = 0.0\n",
    "\n",
    "    # batch size of 64, takes 938 iterations to go through whole dataset\n",
    "    for batch in range(60000//batch_size + 1):\n",
    "        if batch != 60000//batch_size:\n",
    "            this_batch_size = batch_size\n",
    "            input = np.zeros(batch_size*28*28).reshape(batch_size, 28*28)\n",
    "            input = torch.from_numpy(input)\n",
    "            for i in range(batch_size):\n",
    "                input[i] = full_train_data[permutation[batch*batch_size+i]]\n",
    "            input = input.to(torch.float32)\n",
    "\n",
    "            target = np.zeros(batch_size*10).reshape(batch_size, 10)\n",
    "            target = torch.from_numpy(target)\n",
    "            for i in range(batch_size):\n",
    "                target[i] = full_train_target[permutation[batch*batch_size+i]]\n",
    "            target = target.to(torch.float32)\n",
    "\n",
    "        else:\n",
    "            this_batch_size = 60000-batch*batch_size\n",
    "            input = np.zeros(this_batch_size*28 *\n",
    "                             28).reshape(this_batch_size, 28*28)\n",
    "            for i in range(this_batch_size):\n",
    "                input[i] = full_train_data[permutation[batch*batch_size+i]]\n",
    "            input = torch.from_numpy(input)\n",
    "            input = input.to(torch.float32)\n",
    "\n",
    "            target = np.zeros(this_batch_size*10).reshape(this_batch_size, 10)\n",
    "            for i in range(this_batch_size):\n",
    "                target[i] = full_train_target[permutation[batch*batch_size+i]]\n",
    "            target = torch.from_numpy(target)\n",
    "            target = target.to(torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = mlp(input)\n",
    "        loss = criterion(prediction, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_in_this_epoch += loss.item()*this_batch_size\n",
    "\n",
    "    print(\"epoch\", epoch, end=\": \")\n",
    "    print(\"the loss is\", train_loss_in_this_epoch/60000)\n",
    "\n",
    "test_prediction = mlp(full_test_data)\n",
    "test_loss = criterion(test_prediction, full_test_target)\n",
    "print(\"the loss on test data is\", test_loss.item())\n",
    "\n",
    "\n",
    "predicted_numbers = []\n",
    "for i in range(10000):\n",
    "    predicted_number = torch.argmax(test_prediction[i]).item()\n",
    "    predicted_numbers.append(predicted_number)\n",
    "predicted_numbers = np.array(predicted_numbers)\n",
    "\n",
    "num_error = 0\n",
    "for truth, prediction in zip(test_labels, predicted_numbers):\n",
    "    if truth != prediction:\n",
    "        num_error += 1\n",
    "print(\"The accuracy on the testing data is\", 1-num_error/10000)\n",
    "\n",
    "###\n",
    "# test accuract depending on differnet num_of_neurons_in_hidden\n",
    "# 4: the loss on test data is 0.5124706843365774\n",
    "#    The accuracy on the testing data is 0.855\n",
    "# 8: the loss on test data is 0.2843984853701364\n",
    "#    The accuracy on the testing data is 0.9202\n",
    "# 16: the loss on test data is 0.22133442593356567\n",
    "#     The accuracy on the testing data is 0.9357\n",
    "# 32: the loss on test data is 0.1721920902196079\n",
    "#     The accuracy on the testing data is 0.9493\n",
    "# 64: the loss on test data is 0.1563105991184248\n",
    "#     The accuracy on the testing data is 0.9535\n",
    "# 128: the loss on test data is 0.13844306242942367\n",
    "#      The accuracy on the testing data is 0.9589\n",
    "# 256: the loss on test data is 0.1314683234138732\n",
    "#      The accuracy on the testing data is 0.9615\n",
    "\n",
    "# without trainning: the loss on test data is 2.303334325838089\n",
    "#                    The accuracy on the testing data is 0.09989999999999999\n",
    "###\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea7655f8563b6b304d5ebbeaa9fbbcccb7096429f906efd7b44913c32e5b88a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
