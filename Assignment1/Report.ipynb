{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **COMP5214 Assignment 1 Report**\n",
    "Note that except for KNN, during the trainning process of all MLP, CNN, and CAN, I randomenized the order and images for each batch. This means that each time the code is run, the model may be trained in a different way. Therefore, the testing results for each run may have slight differences. This slight difference shall not affect the comparison in terms of overall accuracy and efficiency between different models.\n",
    "## **K Nearest Neighbors (KNN)**\n",
    "For the given `MNIST` Dataset, and the use of the `KNeighborsClassifier` from `sklearn.neighbors`, we obtained the following results:\n",
    "* **The accuray for K=1 is 0.9631.**\n",
    "* The accuracy reaches the highest when K=3, with accuacy of 0.9633.\n",
    "* A plot of accuracy versus K for K from 1 to 10 is shown below:\n",
    "\n",
    "![Accuracy of this KNN classifier for different K](./KNNPlot.png \"Accuracy of this KNN classifier for different K\")\n",
    "\n",
    "In General, the accuracy goes down after reaching the top at K=3.\n",
    "\n",
    "## **Multilayer Perceptron (MLP)**\n",
    "By experimenting with different number of neurons in the hidden layer, we obtained the following plot of accuracy versus the number of neurons in the hidden layer:\n",
    "  \n",
    "![Accuracy under different number of neurons in the hidden layer](./MLP_accuracy.png \"Accuracy under different number of neurons in the hidden layer\")  \n",
    "\n",
    "We also obtained the plot illustrating the trend of loss change:  \n",
    "  \n",
    "![Loss under different number of neurons in the hidden layer](./MLP_loss.png \"Loss under different number of neurons in the hidden layer\")\n",
    "\n",
    "From the graph we can see that the accuracy becomes higher and higher as the number of neurons in the hidden layer doubles. However, the rate of accuracy improvement becomes slower as more and more neurons are added.\n",
    "\n",
    "## **Convolutional Neural Networks (CNN)**\n",
    "**The accuracy on the test data for this network is 0.9698**, which is better than the accuracy of the MLP model when 256 neurons were added in the hidden layers. This suggests that adding convolutional layers before fully connected layers may have better performance in tasks like image classification.\n",
    "\n",
    "## **Context Aggregation Networks (CAN)**\n",
    "For this model, without padding, the input will not be able to pass the 4th convolutional layer whose kernel size will be bigger than the local input.\n",
    "\n",
    "**The Accuracy of the CAN model as illustrated in `table 1` is 0.9807.** Note that for this CAN model, it is observed that the speed of convergence is rather slow, so I increased the learning rate from 0.01(which was the learning rate used in MLP and CNN), to 0.1, to increase the speed of convergence. I experimented this model with 4, 8, 16, 32 feature maps, and obtained the following relationships between the number of feature maps and the accuracy:\n",
    "  \n",
    "![Accuracy under different number of feature maps](./CAN_accuracy.png \"Accuracy under different number of feature maps\")\n",
    "  \n",
    "It is possible that with 32 feature maps and a learning rate of 0.1, it may have over-fitted. Since we simpley uses the cross entropy loss, without adding any regularization, this might be the reason why the performance is better when the number of feature maps is 16. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ea7655f8563b6b304d5ebbeaa9fbbcccb7096429f906efd7b44913c32e5b88a8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
